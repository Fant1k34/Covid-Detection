{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Сегментация изображений\n\nПеред нами стоит задача **сегментации** кт-снимков легких и определения областей поражения от Covid-19. \n\nИсходные данные: **json** с названиями файлов,\n**images** - сами данные, **labels** - разметка: 0 - этот пиксель **НЕ относится** к поврежденному лёгкому, 1 - пиксель **относится** к повреждённому лёгкому\n\nКаждый скан - numpy arrays of shape (512, 512, n_slices)\n\n","metadata":{"id":"G-PlnxqI46_E"}},{"cell_type":"markdown","source":"## 0. Подготовка данных и импорт библиотек","metadata":{"id":"dgTs2S_X7rra"}},{"cell_type":"code","source":"!pip install -U git+https://github.com/qubvel/segmentation_models.pytorch","metadata":{"id":"vLbRzf2inSlc","outputId":"b04447f8-c9ab-4077-abeb-4915e6a469c8","execution":{"iopub.status.busy":"2021-12-10T21:30:13.120123Z","iopub.execute_input":"2021-12-10T21:30:13.120466Z","iopub.status.idle":"2021-12-10T21:30:29.96072Z","shell.execute_reply.started":"2021-12-10T21:30:13.120423Z","shell.execute_reply":"2021-12-10T21:30:29.959626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport nibabel as nib\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport torchvision\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A # Будем использовать для аугментации данных\nimport segmentation_models_pytorch as smp","metadata":{"id":"b1297f61","papermill":{"duration":3.721435,"end_time":"2021-11-20T10:09:14.019841","exception":false,"start_time":"2021-11-20T10:09:10.298406","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-10T21:30:29.96416Z","iopub.execute_input":"2021-12-10T21:30:29.96438Z","iopub.status.idle":"2021-12-10T21:30:38.639453Z","shell.execute_reply.started":"2021-12-10T21:30:29.964353Z","shell.execute_reply":"2021-12-10T21:30:38.638536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # Определяем доступность gpu\n\ndevice = torch.device(device)\nprint(device)","metadata":{"papermill":{"duration":0.030814,"end_time":"2021-11-20T10:09:14.074634","exception":false,"start_time":"2021-11-20T10:09:14.04382","status":"completed"},"tags":[],"id":"47537a2c","outputId":"cb73daf8-bc87-4c75-983d-b4d6a50e72c4","execution":{"iopub.status.busy":"2021-12-10T21:30:38.641084Z","iopub.execute_input":"2021-12-10T21:30:38.641359Z","iopub.status.idle":"2021-12-10T21:30:38.69017Z","shell.execute_reply.started":"2021-12-10T21:30:38.641319Z","shell.execute_reply":"2021-12-10T21:30:38.689465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive # Маунтим диск \n# drive.mount('/content/drive')","metadata":{"id":"LUwqZhbrfrhw","outputId":"728019b4-f54b-40e5-d627-fceb52a19670","execution":{"iopub.status.busy":"2021-12-10T21:30:38.691343Z","iopub.execute_input":"2021-12-10T21:30:38.693367Z","iopub.status.idle":"2021-12-10T21:30:38.708249Z","shell.execute_reply.started":"2021-12-10T21:30:38.693329Z","shell.execute_reply":"2021-12-10T21:30:38.707337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Структура проекта:\n1. core_path - общий путь до проекта. В коллабе это \"./drive/MyDrive/Deep Learning/CovidKaggleTask/\"\n\n2. path = core_path + \"data/data/\" - Путь до самих файлов. В этой директории есть сам json файл, папки images и labels, в которых есть информация о кт-сканах\n\n3. core_path/models - путь до моделей для обучения. В силу больших по объёму данных, необходимо сохранять каждый раз модель. Таким образом в этой папке сохраняются модели после каждой эпохи. lungs_ct_model_1.h5 - корректное название для модели, которая была обучена только на одной эпохе","metadata":{"id":"G42PBCky6Z1-"}},{"cell_type":"code","source":"# core_path = \"./drive/MyDrive/Deep Learning/CovidKaggleTask/\" # Определяем пути до файлов\n# path = core_path + \"data/data/\"","metadata":{"id":"krYtQDHdftDc","execution":{"iopub.status.busy":"2021-12-10T21:30:38.710969Z","iopub.execute_input":"2021-12-10T21:30:38.711265Z","iopub.status.idle":"2021-12-10T21:30:38.720215Z","shell.execute_reply.started":"2021-12-10T21:30:38.71123Z","shell.execute_reply":"2021-12-10T21:30:38.719359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Для каггла\ncore_path = \"../input/tgcovid/\"\npath = core_path + \"data/data/\"","metadata":{"id":"74a1406e","papermill":{"duration":0.038929,"end_time":"2021-11-20T10:09:14.13529","exception":false,"start_time":"2021-11-20T10:09:14.096361","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-10T21:30:38.721734Z","iopub.execute_input":"2021-12-10T21:30:38.722024Z","iopub.status.idle":"2021-12-10T21:30:38.729704Z","shell.execute_reply.started":"2021-12-10T21:30:38.721989Z","shell.execute_reply":"2021-12-10T21:30:38.728727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"0ce360b8","papermill":{"duration":0.020982,"end_time":"2021-11-20T10:09:14.177632","exception":false,"start_time":"2021-11-20T10:09:14.15665","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Работа с датасетом","metadata":{"id":"EoJf5iRY7xX-"}},{"cell_type":"code","source":"from os import listdir\nfrom os.path import isfile, join\nonlyfiles = [f for f in listdir(path + \"images\") if isfile(join(path + \"images\", f))]\nonlyfiles[:5] # Получаем список всех файлов","metadata":{"id":"3ed72e3b","papermill":{"duration":0.040681,"end_time":"2021-11-20T10:09:14.289384","exception":false,"start_time":"2021-11-20T10:09:14.248703","status":"completed"},"tags":[],"outputId":"1b6b9800-6472-4596-ef92-5675e90aa6ee","execution":{"iopub.status.busy":"2021-12-10T21:30:38.732767Z","iopub.execute_input":"2021-12-10T21:30:38.733063Z","iopub.status.idle":"2021-12-10T21:30:38.776657Z","shell.execute_reply.started":"2021-12-10T21:30:38.733033Z","shell.execute_reply":"2021-12-10T21:30:38.775858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"a6f7cfbb","papermill":{"duration":0.027266,"end_time":"2021-11-20T10:09:31.256798","exception":false,"start_time":"2021-11-20T10:09:31.229532","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Определяемся с аугментацией данных. Пытаемся понять, какие преобразования могут быть полезными для данной задачи","metadata":{"id":"Jf8Bj9JN8726"}},{"cell_type":"code","source":"transforms = A.Compose([\n    # Гауссовый шум\n    A.GaussNoise(always_apply=False, p=0.3, var_limit=(1, 5)),\n    # Отражение\n    # A.Flip(always_apply=False, p=0.5),\n    # Рандомное сжатие\n    A.GridDistortion(always_apply=False, p=0.2, num_steps=3, distort_limit=(-0.30000001192092896, 0.30000001192092896), interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None),\n    # Отражение\n    # A.HorizontalFlip(always_apply=False, p=0.5),\n    # Блюр в движении (резкий поворот камеры)\n    A.MotionBlur(always_apply=False, p=0.4, blur_limit=(3, 5)),\n    # Шум\n    A.MultiplicativeNoise(always_apply=False, p=0.3, multiplier=(0.8899999856948853, 1.1699999570846558), per_channel=True, elementwise=True),\n    # Искажение\n    # A.OpticalDistortion(always_apply=False, p=0.5, distort_limit=(-0.5, 0.5), shift_limit=(-0.05000000074505806, 0.05000000074505806), interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None),\n    # Шум\n    # A.RandomGamma(always_apply=False, p=0.5, gamma_limit=(80, 120), eps=1e-07),\n    # Случайный поворот\n    A.VerticalFlip(always_apply=False, p=0.3)\n    ]\n)","metadata":{"id":"1f5b6b52","papermill":{"duration":0.037521,"end_time":"2021-11-20T10:09:31.451017","exception":false,"start_time":"2021-11-20T10:09:31.413496","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-10T21:30:38.777727Z","iopub.execute_input":"2021-12-10T21:30:38.778048Z","iopub.status.idle":"2021-12-10T21:30:38.785378Z","shell.execute_reply.started":"2021-12-10T21:30:38.778012Z","shell.execute_reply":"2021-12-10T21:30:38.784694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms = A.Compose([\n    A.MotionBlur(always_apply=False, p=0.91, blur_limit=(3, 5)),\n    A.VerticalFlip(always_apply=False, p=0.01)\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:30:38.786681Z","iopub.execute_input":"2021-12-10T21:30:38.787085Z","iopub.status.idle":"2021-12-10T21:30:38.794165Z","shell.execute_reply.started":"2021-12-10T21:30:38.787052Z","shell.execute_reply":"2021-12-10T21:30:38.793397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms = A.Compose([\n    A.OneOf([A.GaussNoise(always_apply=False, p=0.25, var_limit=(4, 12)),\n             A.MotionBlur(always_apply=False, p=0.25, blur_limit=(3, 5)),\n             A.Blur(always_apply=False, p=0.25, blur_limit=(3, 5))]),\n    # A.Rotate(always_apply=False, p=0.9, limit=(-15, 15)),\n    A.augmentations.geometric.transforms.Perspective(scale=(0.03, 0.08), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1, always_apply=False, p=1.0)\n])","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:30:38.795384Z","iopub.execute_input":"2021-12-10T21:30:38.797697Z","iopub.status.idle":"2021-12-10T21:30:38.80473Z","shell.execute_reply.started":"2021-12-10T21:30:38.797659Z","shell.execute_reply":"2021-12-10T21:30:38.804037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Испытаем аугментации сейчас","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\ndef blend_with_mask(image, mask): # Функция возвращает картинку с наложенной на неё маской - label, которые являются ковидной штукой\n    image = image.astype(np.float32)\n    min_in = image.min()\n    max_in = image.max()\n    image = (image - min_in) / (max_in - min_in + 1e-8) * 255\n    image = np.dstack((image, image, image)).astype(np.uint8)\n    zeros = np.zeros_like(mask)\n    mask = np.dstack((zeros, zeros, mask * 255)).astype(np.uint8)\n    return Image.blend(\n        Image.fromarray(image),\n        Image.fromarray(mask),\n        alpha=.3\n    )\n\npath_images = os.path.join(path, 'images')\npath_labels = os.path.join(path, 'labels')\nimage = torch.tensor(nib.load(os.path.join(path_images, onlyfiles[0])).get_fdata(), dtype=torch.uint8).transpose(1, 2).transpose(0, 1) # Загружаем конкретный кт-скан по названию из json\nlabel = torch.tensor(nib.load(os.path.join(path_labels, onlyfiles[0][:-4] + \"_mask.nii\")).get_fdata(), dtype=torch.uint8).transpose(1, 2).transpose(0, 1)\nresult_aug = transforms(image=image.numpy(), mask=label.numpy())\nimage = result_aug[\"image\"]\nlabel = result_aug[\"mask\"]\nprint(image.shape)\n\nslices = []\nslices_num = (20, )\nfor idx in slices_num:\n    slices.append(blend_with_mask(\n        image[idx],\n        label[idx]\n    ))    \n\nfigure = plt.figure(figsize=(12, 12))\nfor i, image in enumerate(slices):\n    ax = figure.add_subplot(1, len(slices), i + 1)\n    ax.imshow(slices[i])","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:30:38.80704Z","iopub.execute_input":"2021-12-10T21:30:38.809041Z","iopub.status.idle":"2021-12-10T21:30:40.322611Z","shell.execute_reply.started":"2021-12-10T21:30:38.809007Z","shell.execute_reply":"2021-12-10T21:30:40.321935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.asarray(image).shape","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:30:40.323523Z","iopub.execute_input":"2021-12-10T21:30:40.323758Z","iopub.status.idle":"2021-12-10T21:30:40.331112Z","shell.execute_reply.started":"2021-12-10T21:30:40.323729Z","shell.execute_reply":"2021-12-10T21:30:40.330472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Тут как обычно: создаём класс датасета, в котором прописываем метод __init__(self, список из сканов для трейна, список из сканов для валидации, аугментации), метод длины датасета и получения элемента из датасета","metadata":{"id":"pL5XE4by9JGX"}},{"cell_type":"code","source":"class CovidDataset(Dataset):\n    def __init__(self, X_train_list, X_val_list, transforms, without_covid_max=9999999):\n        # Загружаем сканы кт\n        path_images = os.path.join(path, 'images')\n        path_labels = os.path.join(path, 'labels')\n        # Подгружаем json с инфой по разметке\n        with open(core_path + 'training_data.json', 'r') as f:\n            dict_training = json.load(f)\n\n        self.X_train = [] \n        self.Y_train = []\n        self.X_val = []\n        self.Y_val = []\n        self.transforms = transforms\n        without_covid = 0\n        for entry in tqdm(dict_training):\n            image = nib.load(os.path.join(path_images, entry['image'][:-3])) # Загружаем конкретный кт-скан по названию из json\n            label = nib.load(os.path.join(path_labels, entry['label'][:-3])) # Загружаем лейблы/разметку для кт-скана\n            image = torch.tensor(image.get_fdata(), dtype=torch.uint8).transpose(1, 2).transpose(0, 1) # Меняем размерность с [43, 512, 512]\n            label = torch.tensor(label.get_fdata(), dtype=torch.uint8).transpose(1, 2).transpose(0, 1) # на [512, 512, 43] для всех картинок\n            \n            \n            if entry['image'][:-3] in X_train_list: # Если этот кт-скан в трейне - загружаем его туда\n                for i in range(len(image)): # Пробегаемся по всем слоям в нужном кт-скане image\n                    if label[i].sum() != 0:\n                        self.X_train.append(image[i]) # Добавляем отдельные картинки\n                        self.Y_train.append(label[i])\n                    else:\n                        if without_covid >= without_covid_max:\n                            continue\n                        else:\n                            without_covid += 1\n                            self.X_train.append(image[i]) # Добавляем отдельные картинки\n                            self.Y_train.append(label[i])\n                        \n            else: \n                for i in range(len(image)): # То же самое, но для валидации\n                    if label[i].sum() != 0:\n                        self.X_val.append(image[i])\n                        self.Y_val.append(label[i])\n     \n    \n    \n    def __len__(self):\n        return len(self.X_train)\n    \n    def __getitem__(self, idx):\n        # Делаем случайную аугментацию. Метод делает аугментацию как для image - нашего скана слоя, так и для его разметки\n        # Для начала определяем поворот на угол...\n        degrees = [-35, -30, -25, -20, -15, -10, -5, 0, 5, 10, 15, 20, 25, 30, 35]\n        X = self.X_train[idx]\n        y = self.Y_train[idx]\n        X = X.type(torch.float)\n        y = y.type(torch.float)\n        X = (torch.Tensor(np.array([X.numpy()]) / 255))\n        y = (torch.Tensor(np.array([y.numpy()])))\n        value = random.random()\n        if random.random() > 0.5:\n            value = random.random()\n            if value > 0.5:\n                X = torchvision.transforms.functional.vflip(X)\n                y = torchvision.transforms.functional.vflip(y)\n            else:\n                X = torchvision.transforms.functional.hflip(X)\n                y = torchvision.transforms.functional.hflip(y)\n        value = random.random()\n        if value >= 0.1:\n            degree = random.choice(degrees)\n            X = torchvision.transforms.functional.rotate(X, degree)\n            y = torchvision.transforms.functional.rotate(y, degree)\n        else:\n            pass\n        value = random.random()\n        if value > 0.5:\n            X = torchvision.transforms.RandomPerspective(distortion_scale=0.15, p=0.5, interpolation=2, fill=0)(X)\n            y = torchvision.transforms.RandomPerspective(distortion_scale=0.15, p=0.5, interpolation=2, fill=0)(y)\n        else:\n            pass\n        value = random.random()\n        if value > 0.5:\n            X = torchvision.transforms.GaussianBlur(1)(X)\n            y = torchvision.transforms.GaussianBlur(1)(y)\n        else:\n            pass\n        \n        # sl = self.transforms(image=X.numpy(), label=y.numpy())\n        return torch.Tensor(X), torch.Tensor(y) # Важно, нельзя передать просто картинку (512, 512), так как используется свёртка по многим измерениям. Необходимо передать в формате\n                                            # [палитра, ширина, высота] - [1, 512, 512]\n    def get_validation_set(self):\n        return (self.X_val, self.Y_val)","metadata":{"id":"5a9b3f05","papermill":{"duration":0.042255,"end_time":"2021-11-20T10:09:31.521089","exception":false,"start_time":"2021-11-20T10:09:31.478834","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-10T21:30:40.332651Z","iopub.execute_input":"2021-12-10T21:30:40.333101Z","iopub.status.idle":"2021-12-10T21:30:40.360604Z","shell.execute_reply.started":"2021-12-10T21:30:40.333064Z","shell.execute_reply":"2021-12-10T21:30:40.359829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 8\n\nnp.random.shuffle(onlyfiles) # Перемешаем названия файлов в случайном порядке (для генерации трэйна и валидации)\ndataset = CovidDataset(onlyfiles[40:], onlyfiles[34:], None, 50)\nloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"id":"4abf5767","papermill":{"duration":2.154384,"end_time":"2021-11-20T10:09:33.70309","exception":false,"start_time":"2021-11-20T10:09:31.548706","status":"completed"},"tags":[],"outputId":"c95dc0ef-3305-4e7c-ed9e-fccbd47139a9","execution":{"iopub.status.busy":"2021-12-10T21:30:40.36485Z","iopub.execute_input":"2021-12-10T21:30:40.365043Z","iopub.status.idle":"2021-12-10T21:30:57.343604Z","shell.execute_reply.started":"2021-12-10T21:30:40.365013Z","shell.execute_reply":"2021-12-10T21:30:57.342883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"0piNIOuqoEb6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Определяемся со структурой нейронной сети\n\nВ данном случае, была выбрана сеть Unet (https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/)\n\nОна отлично подходит для анализа медицинских изображений, так как в ней есть множество skip-connection для решения проблемы затухания градиента (vanishing gradient), а также увеличения информации для следующих слоёв, которые отвечают за выбор признаков (т. е. decoder)","metadata":{"id":"1Im-MvYF9cPa"}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass Unet(nn.Module): # Определим структуру нейронной сети Unet\n    def block_down(self, in_features, out_features):\n        return nn.Sequential(*[nn.Conv2d(in_features, out_features, (3, 3), padding=1),\n                              nn.ReLU(),\n                              nn.BatchNorm2d(out_features)])\n    \n    def block_up(self, in_features, out_features):\n        return nn.Sequential(*[nn.Conv2d(in_features, out_features, (3, 3), padding=1),\n                              nn.ReLU(),\n                              nn.BatchNorm2d(out_features)])\n    \n    \n    def __init__(self):\n        super(Unet, self).__init__()\n        self.block_up11 = self.block_down(1, 32)\n        self.block_up12 = self.block_down(32, 32)\n        self.max_pooling11 = nn.MaxPool2d((2, 2), stride=(2, 2))\n        \n        self.block_up21 = self.block_down(32, 64)\n        self.block_up22 = self.block_down(64, 64)\n        self.max_pooling22 = nn.MaxPool2d((2, 2), stride=(2, 2))\n        \n        self.block_up31 = self.block_down(64, 128)\n        self.block_up32 = self.block_down(128, 128)\n        self.max_pooling33 = nn.MaxPool2d((2, 2), stride=(2, 2))\n        \n        self.block_up41 = self.block_down(128, 256)\n        self.block_up42 = self.block_down(256, 256)\n        self.max_pooling44 = nn.MaxPool2d((2, 2), stride=(2, 2))\n        \n        self.block_up51 = self.block_down(256, 512)\n        self.block_up52 = self.block_down(512, 512)\n        \n        self.block_up61 = nn.Upsample(scale_factor=2)\n        self.block_up62 = self.block_up(512, 256)\n        self.block_up63 = self.block_up(512, 256)\n        self.block_up64 = self.block_up(256, 256)\n        \n        self.block_up71 = nn.Upsample(scale_factor=2)\n        self.block_up72 = self.block_up(256, 128)\n        self.block_up73 = self.block_up(256, 128)\n        self.block_up74 = self.block_up(128, 128)\n        \n        self.block_up81 = nn.Upsample(scale_factor=2)\n        self.block_up82 = self.block_up(128, 64)\n        self.block_up83 = self.block_up(128, 64)\n        self.block_up84 = self.block_up(64, 64)\n        \n        self.block_up91 = nn.Upsample(scale_factor=2)\n        self.block_up92 = self.block_up(64, 32)\n        self.block_up93 = self.block_up(64, 32)\n        self.block_up94 = self.block_up(32, 32)\n        \n        self.block_up100 = self.block_up(32, 1) \n        \n    \n    def forward(self, x):\n        out = self.block_up11(x)\n        out = self.block_up12(out)\n        \n        save1 = out.clone()\n        \n        out = self.max_pooling11(out)\n        \n        out = self.block_up21(out)\n        out = self.block_up22(out)\n        \n        save2 = out.clone()\n        \n        out = self.max_pooling22(out)\n        \n        out = self.block_up31(out)\n        out = self.block_up32(out)\n        \n        save3 = out.clone()\n        \n        out = self.max_pooling33(out)\n        \n        out = self.block_up41(out)\n        out = self.block_up42(out)\n        \n        save4 = out.clone()\n        \n        out = self.max_pooling44(out)\n        \n        out = self.block_up51(out)\n        out = self.block_up52(out)\n        \n        \n        out = self.block_up61(out)\n        out = self.block_up62(out)\n        out = self.block_up63(torch.cat((out, save4), 1))\n        out = self.block_up64(out)\n\n        out = self.block_up71(out)\n        out = self.block_up72(out)\n        out = self.block_up73(torch.cat((out, save3), 1))\n        out = self.block_up74(out)\n\n        out = self.block_up81(out)\n        out = self.block_up82(out)\n        out = self.block_up83(torch.cat((out, save2), 1))\n        out = self.block_up84(out)\n\n        out = self.block_up91(out)\n        out = self.block_up92(out)\n        out = self.block_up93(torch.cat((out, save1), 1))\n        out = self.block_up94(out)\n\n        out = self.block_up100(out)\n        out = nn.Sigmoid()(out)\n        \n        return out","metadata":{"id":"6ba27b26","papermill":{"duration":0.050408,"end_time":"2021-11-20T10:09:33.782202","exception":false,"start_time":"2021-11-20T10:09:33.731794","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-10T21:30:57.34493Z","iopub.execute_input":"2021-12-10T21:30:57.345296Z","iopub.status.idle":"2021-12-10T21:30:57.369899Z","shell.execute_reply.started":"2021-12-10T21:30:57.345261Z","shell.execute_reply":"2021-12-10T21:30:57.369049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"7cdd7a1d","papermill":{"duration":0.027521,"end_time":"2021-11-20T10:09:33.838013","exception":false,"start_time":"2021-11-20T10:09:33.810492","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Также в качестве Лосса возьмём взвешенную кросс-энтропию (focal loss), потому что она позволяет учесть дисбаланс классов","metadata":{"id":"dk0IUVaR-HPY"}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef dice_loss(inputs: torch.Tensor, targets: torch.Tensor):\n    inp = inputs.contiguous().view(-1)\n    tar = targets.contiguous().view(-1)\n    noise = random.randint(1, 1000) / 10000000000\n    \n    return 1 - ((2 * (inp * tar).sum() + noise) / ((inp).sum() + (tar).sum() + noise))","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:30:57.371157Z","iopub.execute_input":"2021-12-10T21:30:57.371411Z","iopub.status.idle":"2021-12-10T21:30:57.383458Z","shell.execute_reply.started":"2021-12-10T21:30:57.371379Z","shell.execute_reply":"2021-12-10T21:30:57.382773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TverskyLoss(nn.Module):\n    def __init__(self, alpha=0.7):\n        super(TverskyLoss, self).__init__()\n        self.alpha = alpha\n\n    def forward(self, inputs, targets, smooth=1):\n        y_pred = inputs\n        y_true = targets\n        y_true_pos = y_true.view(-1)\n        y_pred_pos = y_pred.view(-1)\n        true_pos = torch.sum(y_true_pos * y_pred_pos)\n        false_neg = torch.sum(y_true_pos * (1 - y_pred_pos))\n        false_pos = torch.sum((1 - y_true_pos) * y_pred_pos)\n        return 1 - (true_pos + smooth) / (true_pos + self.alpha * false_neg + (1 - self.alpha) * false_pos + smooth)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:30:57.385956Z","iopub.execute_input":"2021-12-10T21:30:57.386818Z","iopub.status.idle":"2021-12-10T21:30:57.394279Z","shell.execute_reply.started":"2021-12-10T21:30:57.386779Z","shell.execute_reply":"2021-12-10T21:30:57.393634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n\n\ndef sigmoid_focal_loss(\n    inputs: torch.Tensor,\n    targets: torch.Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2,\n    reduction: str = \"none\"):\n    \"\"\"\n    Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py .\n    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n\n    Args:\n        inputs: A float tensor of arbitrary shape.\n                The predictions for each example.\n        targets: A float tensor with the same shape as inputs. Stores the binary\n                classification label for each element in inputs\n                (0 for the negative class and 1 for the positive class).\n        alpha: (optional) Weighting factor in range (0,1) to balance\n                positive vs negative examples or -1 for ignore. Default = 0.25\n        gamma: Exponent of the modulating factor (1 - p_t) to\n               balance easy vs hard examples.\n        reduction: 'none' | 'mean' | 'sum'\n                 'none': No reduction will be applied to the output.\n                 'mean': The output will be averaged.\n                 'sum': The output will be summed.\n    Returns:\n        Loss tensor with the reduction option applied.\n    \"\"\"\n    p = torch.sigmoid(inputs)\n    ce_loss = F.binary_cross_entropy_with_logits(\n        inputs, targets, reduction=\"none\"\n    )\n    p_t = p * targets + (1 - p) * (1 - targets)\n    loss = ce_loss * ((1 - p_t) ** gamma)\n\n    if alpha >= 0:\n        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n        loss = alpha_t * loss\n\n    if reduction == \"mean\":\n        loss = loss.mean()\n    elif reduction == \"sum\":\n        loss = loss.sum()\n\n    return loss\n","metadata":{"id":"1bab41f0","papermill":{"duration":0.042374,"end_time":"2021-11-20T10:09:33.963912","exception":false,"start_time":"2021-11-20T10:09:33.921538","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-10T21:30:57.395707Z","iopub.execute_input":"2021-12-10T21:30:57.395951Z","iopub.status.idle":"2021-12-10T21:30:57.406274Z","shell.execute_reply.started":"2021-12-10T21:30:57.395921Z","shell.execute_reply":"2021-12-10T21:30:57.405566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"999a3bbf","papermill":{"duration":0.027423,"end_time":"2021-11-20T10:09:34.019205","exception":false,"start_time":"2021-11-20T10:09:33.991782","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Обучение модели","metadata":{"id":"ee5836ef","papermill":{"duration":0.028007,"end_time":"2021-11-20T10:09:34.075764","exception":false,"start_time":"2021-11-20T10:09:34.047757","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-10T16:09:36.503126Z","iopub.execute_input":"2021-12-10T16:09:36.503391Z","iopub.status.idle":"2021-12-10T16:10:18.211861Z","shell.execute_reply.started":"2021-12-10T16:09:36.503362Z","shell.execute_reply":"2021-12-10T16:10:18.210917Z"}}},{"cell_type":"code","source":"import segmentation_models_pytorch as smp","metadata":{"id":"dcf94792","papermill":{"duration":0.141974,"end_time":"2021-11-20T10:09:34.246774","exception":false,"start_time":"2021-11-20T10:09:34.1048","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-10T21:30:57.407483Z","iopub.execute_input":"2021-12-10T21:30:57.40863Z","iopub.status.idle":"2021-12-10T21:30:57.41617Z","shell.execute_reply.started":"2021-12-10T21:30:57.408579Z","shell.execute_reply":"2021-12-10T21:30:57.415446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_previous_versions = False\nprevious_i = 0\npath_to_model = \"output/kaggle/working/\"\n\nif use_previous_versions:\n    models_variation = []\n    for put, papki, files in os.walk(\".\"):\n        for el in files:\n            if \"lungs_ct_model\" in el:\n                models_variation.append(el)\n    # Название файла - lungs_ct_model_1.h5\n    if len(models_variation) != 0:\n        \n        models_variation = sorted(models_variation, key=lambda x: - int(x.split(\"_\")[-1].split(\".\")[0]))\n        model = torch.load(models_variation[-1])\n        previous_i = int(models_variation[0].split(\"_\")[-1].split(\".\")[0])\n        print(models_variation)\n        print(\"Загружена прошлая модель Unet: {}\".format(str(previous_i)))\n    else:\n        # model = UNet(1, in_channels=1, depth=5, \n        #          start_filts=64, up_mode='transpose', \n        #          merge_mode='concat')\n        model = Unet()\n        print(\"Нет предобученных моделей\")\nelse:\n    model = smp.UnetPlusPlus(encoder_name='resnet18', in_channels=1, classes=1, activation=\"sigmoid\")\n    print(\"Загружен непредобученный Unet\")\n\n# lambda1 = lambda epoch: 8839103922077863 ** epoch\n# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n\ndevice = torch.device('cuda:0')\n\nmodel = model.to(device)","metadata":{"id":"b937f945","papermill":{"duration":0.027474,"end_time":"2021-11-20T10:09:34.302345","exception":false,"start_time":"2021-11-20T10:09:34.274871","status":"completed"},"tags":[],"outputId":"350806e8-f70b-4f37-efdc-4a4ad5383e89","execution":{"iopub.status.busy":"2021-12-10T21:30:57.417441Z","iopub.execute_input":"2021-12-10T21:30:57.417719Z","iopub.status.idle":"2021-12-10T21:31:03.906831Z","shell.execute_reply.started":"2021-12-10T21:30:57.417685Z","shell.execute_reply":"2021-12-10T21:31:03.906072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epoch = 175\nlr = 0.0005\n\ndice_loss_criterion = dice_loss\nfocal_loss_criterion = sigmoid_focal_loss\ntverskoy_loss = TverskyLoss(alpha=0.7)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, threshold=0.035, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:54:09.064385Z","iopub.execute_input":"2021-12-10T21:54:09.065072Z","iopub.status.idle":"2021-12-10T21:54:09.079937Z","shell.execute_reply.started":"2021-12-10T21:54:09.065034Z","shell.execute_reply":"2021-12-10T21:54:09.078986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Unet -> Unet++\n2. Add scheduler\n3. Add validation_check\n4. Loss (?)\n5. Оформить","metadata":{"id":"S6lIwPwyzd6k","execution":{"iopub.status.busy":"2021-12-10T12:30:29.460451Z","iopub.execute_input":"2021-12-10T12:30:29.460728Z","iopub.status.idle":"2021-12-10T12:30:29.466215Z","shell.execute_reply.started":"2021-12-10T12:30:29.460677Z","shell.execute_reply":"2021-12-10T12:30:29.46538Z"}}},{"cell_type":"code","source":"class ValidDataset(Dataset):\n    def __init__(self, X, Y):\n        # Загружаем сканы кт\n        self.X = X\n        self.Y = Y     \n    \n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        # Делаем случайную аугментацию. Метод делает аугментацию как для image - нашего скана слоя, так и для его разметки\n        # Для начала определяем поворот на угол...\n        degrees = [-30, -25, -20, -15, -10, -5, 0, 5, 10, 15, 20, 25, 30]\n        X = self.X[idx]\n        y = self.Y[idx]\n        X = X.type(torch.float)\n        y = y.type(torch.float)\n        X = (torch.Tensor(np.array([X.numpy()]) / 255))\n        y = (torch.Tensor(np.array([y.numpy()])))\n        value = random.random()\n        if random.random() > 0.5:\n            value = random.random()\n            if value > 0.5:\n                X = torchvision.transforms.functional.vflip(X)\n                y = torchvision.transforms.functional.vflip(y)\n            else:\n                X = torchvision.transforms.functional.hflip(X)\n                y = torchvision.transforms.functional.hflip(y)\n        value = random.random()\n        if value >= 0.1:\n            degree = random.choice(degrees)\n            X = torchvision.transforms.functional.rotate(X, degree)\n            y = torchvision.transforms.functional.rotate(y, degree)\n        else:\n            pass\n        value = random.random()\n        if value > 0.5:\n            X = torchvision.transforms.RandomPerspective(distortion_scale=0.15, p=0.5, interpolation=2, fill=0)(X)\n            y = torchvision.transforms.RandomPerspective(distortion_scale=0.15, p=0.5, interpolation=2, fill=0)(y)\n        else:\n            pass\n        value = random.random()\n        if value > 0.5:\n            X = torchvision.transforms.GaussianBlur(1)(X)\n            y = torchvision.transforms.GaussianBlur(1)(y)\n        else:\n            pass\n        \n        # sl = self.transforms(image=X.numpy(), label=y.numpy())\n        return torch.Tensor(X), torch.Tensor(y) # Важно, нельзя передать просто картинку (512, 512), так как используется свёртка по многим измерениям. Необходимо передать в формате\n                                            # [палитра, ширина, высота] - [1, 512, 512]","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:33:08.901665Z","iopub.execute_input":"2021-12-10T21:33:08.902186Z","iopub.status.idle":"2021-12-10T21:33:08.915325Z","shell.execute_reply.started":"2021-12-10T21:33:08.902149Z","shell.execute_reply":"2021-12-10T21:33:08.91466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validation_score(model):\n    val_tverskoy_losses = []\n    valid_dataset = ValidDataset(*dataset.get_validation_set())\n    valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)\n    \n    for X, Y in valid_loader:\n        X = X.to(device)\n        Y = Y.to(device)\n        output = model.forward(X)\n        val_tverskoy_losses.append(tverskoy_loss(output, Y).item())\n    \n    del valid_dataset\n    del valid_loader\n    \n    return sum(val_tverskoy_losses) / len(val_tverskoy_losses)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:33:09.845997Z","iopub.execute_input":"2021-12-10T21:33:09.84674Z","iopub.status.idle":"2021-12-10T21:33:09.85297Z","shell.execute_reply.started":"2021-12-10T21:33:09.846691Z","shell.execute_reply":"2021-12-10T21:33:09.852012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses = []\n\nfor epoch in range(num_epoch):\n    epoch_losses = []\n    dice_losses = []\n    focal_losses = []\n        \n    for X, Y in tqdm(loader):\n        X = X.to(device)\n        Y = Y.to(device)\n\n        optimizer.zero_grad()\n        output = model(X)\n        \n        # dice = dice_loss(output, Y)\n        # focal = focal_loss_criterion(output, Y, 0.25, 2, \"mean\")\n        loss = tverskoy_loss(output, Y)\n        loss.backward()\n        # print(list(model.parameters()))\n        clip_grad_norm_(model.parameters(), 99999)\n        \n        optimizer.step()\n\n        del X\n        del Y\n        torch.cuda.empty_cache()\n        epoch_losses.append(loss.item())\n        # dice_losses.append(dice.item())\n        # focal_losses.append(focal.item())\n    \n    # val_loss = validation_score(model)\n    common_loss = sum(epoch_losses)/len(epoch_losses)\n    scheduler.step(common_loss)\n    \n    # Выводим и сохраняем Лосс\n    # print(\"Mean Dice Loss: {}\".format(str(sum(dice_losses)/len(dice_losses))))\n    # print(\"Mean Focal Loss: {}\".format(str(sum(focal_losses)/len(focal_losses))))\n    print(\"Tversky Loss: {}\".format(str(common_loss)))\n    # print(\"Validation Loss: {}\".format(val_loss))\n    losses.append(sum(epoch_losses)/len(epoch_losses))\n    \n    torch.save(model, \"lungs_ct_model_\" + str(epoch + previous_i) + \".h5\")\n    print(\"Saved model {}\".format(epoch + previous_i))","metadata":{"id":"13acb9a6","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"outputId":"8ddeb7fc-e25f-40d4-ef0a-9a721f2721be","execution":{"iopub.status.busy":"2021-12-10T21:31:03.985096Z","iopub.execute_input":"2021-12-10T21:31:03.985566Z","iopub.status.idle":"2021-12-10T21:32:56.952107Z","shell.execute_reply.started":"2021-12-10T21:31:03.985531Z","shell.execute_reply":"2021-12-10T21:32:56.950909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"9rRPcgOAk9y5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"HxWD3zUPk97s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"7db80ca5","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"df8be4a0","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"22a46a27","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8a572614","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"0454ea49","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"970bf748","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"f48d3dd7","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"outputId":"af6aa01d-ac99-4ae5-ed70-4e2af4fda595","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Fkhy0KBy8ea5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"kEqA7ew18edn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"LRwYizKL8fTU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Визуализация","metadata":{"id":"1C1JAvvK8gKO"}},{"cell_type":"code","source":"valid_dataset = ValidDataset(*dataset.get_validation_set())\nvalid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:33:14.028777Z","iopub.execute_input":"2021-12-10T21:33:14.029348Z","iopub.status.idle":"2021-12-10T21:33:14.033478Z","shell.execute_reply.started":"2021-12-10T21:33:14.029289Z","shell.execute_reply":"2021-12-10T21:33:14.032549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize some of the slices\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef blend(image, mask): # Функция возвращает картинку с наложенной на неё маской - label, которые являются ковидной штукой\n    print(image)\n    image = image.astype(np.float32)\n    min_in = image.min()\n    max_in = image.max()\n    image = (image - min_in) / (max_in - min_in + 1e-8) * 255\n    image = np.dstack((image, image, image)).astype(np.uint8)\n    zeros = np.zeros_like(mask)\n    mask = np.dstack((zeros, zeros, mask * 255)).astype(np.uint8)\n    return Image.blend(\n        Image.fromarray(image),\n        Image.fromarray(mask),\n        alpha=.3\n    )\n\nslices_num = (7, )\nslices = []\nfor idx in slices_num:\n    k = valid_dataset[idx]\n    slices.append(blend(\n        k[0][0].numpy(),\n        k[1][0].numpy()\n    ))\n    prediction = model.forward(k[0].view(1, 1, 512, 512).to(device)).cpu().detach().transpose(0, 1).transpose(1, 2).transpose(2, 3)[0]\n    prediction[prediction >= 0.85] = 1\n    prediction[prediction < 0.85] = 0\n    slices.append(\n        torch.cat([prediction, prediction, prediction], 2)\n    )\n    print(prediction[(prediction > 0.5) & (prediction < 1)])\n\nfigure = plt.figure(figsize=(18, 18))\nfor i, image in enumerate(slices):\n    ax = figure.add_subplot(1, len(slices), i + 1)\n    ax.imshow(slices[i])","metadata":{"id":"pJ_UA6zE8fXp","outputId":"a1451820-596a-44b2-a3d1-26a897bc1b54","execution":{"iopub.status.busy":"2021-12-10T21:39:00.246777Z","iopub.execute_input":"2021-12-10T21:39:00.247052Z","iopub.status.idle":"2021-12-10T21:39:01.074129Z","shell.execute_reply.started":"2021-12-10T21:39:00.247024Z","shell.execute_reply":"2021-12-10T21:39:01.073425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"b3wyrFlTCaTP","outputId":"0c6ab455-d967-43dc-d1cd-e4c1de4a0b3a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"2JMszkDaBVhg","outputId":"de9ebcfe-b48f-4385-b110-cf0fea86f690","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"FVtWkPSI8egB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Inference","metadata":{}},{"cell_type":"code","source":"\"\"\"\nLoad testing data into images and labels lists\n\nimages list consists of CT scans -  numpy arrays of shape (512, 512, n_slices)\n\"\"\"\nwith open(core_path + 'testing_data.json', 'r') as f:\n    dict_testing = json.load(f)\n\nimages_testing = []\nlabel_testing = []\nfor entry in tqdm(dict_testing):\n    image = nib.load(os.path.join(path + \"images/\", entry['image'][:-3]))\n    images_testing.append(image.get_fdata())","metadata":{"id":"ef283a11","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2021-12-10T21:53:47.635527Z","iopub.execute_input":"2021-12-10T21:53:47.636055Z","iopub.status.idle":"2021-12-10T21:53:47.719638Z","shell.execute_reply.started":"2021-12-10T21:53:47.635971Z","shell.execute_reply":"2021-12-10T21:53:47.71854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresh = 0.99\nn_id = len(images_testing)\n\nfor i in range(n_id):\n    n_imgs = images_testing[i][1].shape[-1]\n    name = images_testing[i][0]\n    \n    for j in range(n_imgs):\n        inp = images_testing[i][:, :, j]\n\n        inp = torch.from_numpy(np.array(inp))\n        inp = inp.to(device)\n        inp = inp.type(torch.float)\n        inp = inp.view(-1, 1, 512, 512)\n\n        res = model(inp)\n        res = res.cpu()\n        outp = res.detach().numpy()[0][0]\n\n        outp /= np.max(outp)\n        outp[outp >= thresh] = 1\n        outp[outp < thresh] = 0\n\n        label_testing.append(outp)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:40:01.395195Z","iopub.execute_input":"2021-12-10T21:40:01.395458Z","iopub.status.idle":"2021-12-10T21:40:13.720244Z","shell.execute_reply.started":"2021-12-10T21:40:01.395424Z","shell.execute_reply":"2021-12-10T21:40:13.71944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nWrite your code here\n\nYou need to:\n 1. Predict labels for CT scans from images list\n 2. Store them in the labels_predicted list in form of numpy arrays of shape (512, 512, n_slices), where:\n    0 - background class\n    1 - regions of consolidation class\n\"\"\"\n# model = model.to(\"cpu\")\n# labels_predicted = []\n# for ct in images_predicted:\n#     for ct_slice in ct.swapaxes(2, 1).swapaxes(1, 0):\n#         label = model.forward(torch.tensor(np.array([[ct_slice]]), dtype=torch.float32))\n#         try: \n#             ct_label = torch.cat((ct_label, label), 0)\n#         except:\n#             ct_label = torch.tensor(label)","metadata":{"id":"3fcf0298","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2021-12-10T21:40:16.473914Z","iopub.execute_input":"2021-12-10T21:40:16.47429Z","iopub.status.idle":"2021-12-10T21:40:16.480223Z","shell.execute_reply.started":"2021-12-10T21:40:16.474256Z","shell.execute_reply":"2021-12-10T21:40:16.479498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize some of the predictions\n\n# patient_num = 5\n# slices_num = (10, 20, 30)\n# slices = []\n# for idx in slices_num:\n#     slices.append(blend(\n#         images_testing[patient_num][..., idx],\n#         label_testing[patient_num][..., idx]\n#     ))\n\n# figure = plt.figure(figsize=(18, 18))\n# for i, image in enumerate(slices):\n#     ax = figure.add_subplot(1, len(slices), i + 1)\n#     ax.imshow(slices[i])","metadata":{"id":"06ed33d7","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2021-12-10T21:40:18.73765Z","iopub.execute_input":"2021-12-10T21:40:18.738186Z","iopub.status.idle":"2021-12-10T21:40:18.74263Z","shell.execute_reply.started":"2021-12-10T21:40:18.738149Z","shell.execute_reply":"2021-12-10T21:40:18.74191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"3b8db706","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_encoding(x):\n    dots = np.where(x.T.flatten() >= 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b > prev + 1):\n            run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return [str(item) for item in run_lengths]","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:40:21.402689Z","iopub.execute_input":"2021-12-10T21:40:21.402976Z","iopub.status.idle":"2021-12-10T21:40:21.408245Z","shell.execute_reply.started":"2021-12-10T21:40:21.402945Z","shell.execute_reply":"2021-12-10T21:40:21.407581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nwith open(f'{core_path}testing_data.json', 'r') as f:\n            dict_testing = json.load(f)\n\n# output = []\n# for entry in tqdm(dict_testing):\n#     image = nib.load(os.path.join(f'{core_path}data/data/images', entry['image'][:-3]))\n#     image = image.get_fdata().swapaxes(0, 2).swapaxes(1, 2).reshape(-1, 1, 512, 512)\n#     tmp = None\n#     for i in range(image.shape[0]):  \n#         img = image[i:i+1]\n#         img = torch.tensor(img).to(device).float()\n#         img = model(img).cpu().detach().numpy()[0]\n#         tmp = img if tmp is None else np.vstack([tmp, img])\n\n#     tmp = tmp.swapaxes(0, 1).swapaxes(1, 2)\n#     output.append(tmp)\n# print(output[0][0])\n\nwith open(f'submission.csv', \"wt\") as sb:\n    submission_writer = csv.writer(sb, delimiter=',')\n    submission_writer.writerow([\"Id\", \"Predicted\"])\n    for k_i, patient_i in tqdm(zip(dict_testing, label_testing)):\n        submission_writer.writerow([\n                f\"{k_i['image'][:-7]}\",\n                \" \".join(rle_encoding(patient_i))\n            ])","metadata":{"execution":{"iopub.status.busy":"2021-12-10T21:40:24.59915Z","iopub.execute_input":"2021-12-10T21:40:24.59942Z","iopub.status.idle":"2021-12-10T21:40:24.683704Z","shell.execute_reply.started":"2021-12-10T21:40:24.599384Z","shell.execute_reply":"2021-12-10T21:40:24.682912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}